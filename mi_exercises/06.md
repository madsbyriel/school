# Lecture 6

# Exercise 1

Suppose you want to use a neural network for predicting user preferences based on the data set in the table below.

![[datatable_06.png]]

What neural network structure could you use, especially: what would be the input and output units?

In this case, the input units would be a vector with a length of 4, each element having the domain of $\{0, 1\}$. The output would be similar, just a single bit for knowing whether the user has skipped or read the article. You could probably also output a probability of whether the user has read an article or not, but strictly speaking that wouldn't be a binary unit like the target feature.

# Exercise 2

Compute for the neural network below the **Rating** output computed for the two inputs

| Fat | Sugars |
| - | - |
| 1 | 5 |
| 0 | 14 |

The two hidden units have the sigmoid activation function. Values for this function can be either computed precisely according to the definition $\sigma(x) = 1/(1 + e −x)$, or you can read approximate function values off the plot on the right below. The output unit has the identity activation function, i.e. the output is just the weighted sum of the inputs.

![[ai1_06.png]]

We must assume that this model does not have a bias since no weight can be determined (or a bias of 0). We determine the weights for each of the neurons:

$$
\begin{align*}
    n_1 &= \{0, 1.3, -0.3\} \\
    n_2 &= \{0, 1.7, 1.0\} \\
    o &= \{0, 0.6, 1.4\} \\
\end{align*}
$$

Now we can calculate the two passes through the model (note that the data point contains a prepended $1$, which is for the bias, which would be 0, essentially, it doesn't matter):

$$
    \begin{bmatrix}
            1 & 1 & 5 \\
            1 & 0 & 14 \\
        \end{bmatrix} * \begin{bmatrix}
            0 & 0 \\
            1.3 & 1.7 \\
            -0.3 & 1.0 \\
    \end{bmatrix} = 
    \begin{bmatrix}
        -0.2 & 6.7 \\
        -4.2 & 14.0 \\
    \end{bmatrix}
$$

The first column of the resulting matrix contains the values for the first neuron given inputs $(1, 5)$ and $(0, 14)$, respectively. The second column likewise, but for the second neuron, which has different weights. Now we need to remember to apply the sigmoid function to these two neurons (we use a simplified syntax, imagine iterating on each element, applying sigmoid):

$$
    \sigma(\begin{bmatrix}
        -0.2 & 6.7 \\
        -4.2 & 14.0 \\
    \end{bmatrix}) = 
    \begin{bmatrix}
        0.45 & 0.99 \\
        0.015 & 0.99 \\
    \end{bmatrix}
$$

Now finally we can use the computed values to determine the output. We once again have a vector with weights and the values computed previously will be the data points.

$$
\begin{bmatrix}
        0.45 & 0.99 \\
        0.015 & 0.99 \\
\end{bmatrix} *
\begin{bmatrix}
    0.6 \\
    -1.4 \\
\end{bmatrix} = 
\begin{bmatrix}
    -1.13 \\
    -1.40 \\
 \end{bmatrix}
$$

The resulting computation is a 2-vector where the first element, the topmost, is the result of the first pass through the network with values (1, 5), similarly for the second element, but for the second pass with values (0, 14).

# Exercise 3

Assume that we have the following training examples:

| $X_1$ | $X_2$ | T |
| - | - | - |
| 1 | 1 | 1 |
| -1 | 1 | -1 |
| 1 | -1 | 1 |
| -1 | -1 | -1 |

That is, with input $X_1 = 1$ and $X_2 = −1$ we want the output $1$.

Consider a perceptron (single neuron) with threshold input $1$ and with initial weights $(w_0 = 0, w_1 = 0, w_2 = 0)$. 

 - Show the first two iterations of gradient descent when learning a perceptron (having the sign function as activation function) using learning rate α = 0.25. Indicate the intermediate steps that need to be computed (e.g. the value of forward propagation, error term and how the weights are adjusted).

To do this, we first calculate the output given the inputs above, and from this output we calculate the error, such that we can adjust the weights accordingly.

As noted in previous exercise, the first weight indicates the bias of the perceptron, whereas the two other weights indicate the weight of the input given for $X_1$ and $X_2$, respectively. In any case, they are all 0, so the output of this model is easily predicted, however, the formal calculation is:

$$
\begin{bmatrix}
    1 & 1 & 1 \\
    1 & -1 & 1 \\
    1 & 1 & -1 \\
    1 & -1 & -1 \\
\end{bmatrix} * 
\begin{bmatrix}
    0 \\
    0 \\
    0 \\
\end{bmatrix} =
\begin{bmatrix}
    0 \\
    0 \\
    0 \\
    0 \\
\end{bmatrix}
$$

We must remember to apply the activation function (sign function) to this output (we use a simplified syntax again):

$$
sign(\begin{bmatrix}
    0 \\
    0 \\
    0 \\
    0 \\
\end{bmatrix}) = 
\begin{bmatrix}
    -1.0 \\
    -1.0 \\
    -1.0 \\
    -1.0 \\
\end{bmatrix}
$$

From this we can compute the error (which is just the output subtracted from the target value):

$$
\begin{bmatrix}
    -1.0 \\
    -1.0 \\
    -1.0 \\
    -1.0 \\
\end{bmatrix} -
\begin{bmatrix}
    1 \\
    -1 \\
    1 \\
    -1 \\
\end{bmatrix} =
\begin{bmatrix}
    -2. \\
     0. \\
    -2. \\
     0. \\
\end{bmatrix}
$$

Now that we have the error we compute the amount we need to change the current weights based on the formula:

$$
w' = w - 
$$

![[formula_06_02.png]]






























