# Exercise 1:

Consider that we have the following data-points, which we want to fit using linear regression:

| $x_1$ | $x_2$ | $y$ |
| ----- | ----- | --- |
| 1     | 1     | 2   |
| 0     | 0     | 0   |
| 1     | 0     | 3   |
| 2     | 1     | 2   |

What is the Mean Squared Error (MSE) error for the following sets of parameters:

- $w0 = 0$, $w1 = 1$, $w2 = âˆ’1$
- $w0 = 1$, $w1 = 1$, $w2 = 0$

We realise that this is simply a linear transformation and we use matrix multiplication to solve this elegantly. Instead of having only 2 data points, the tuple $(x_1, x_2)$, we introduce $x_0$ which for all points is 1, resulting in a three-tuple as a data point.

$$
\hat{y} = \begin{bmatrix}
    1 & 1 & 1 \\
    1 & 0 & 0 \\
    1 & 1 & 0 \\
    1 & 2 & 1 \\
\end{bmatrix} * \begin{bmatrix}
    0 & 1 \\
    1 & 1 \\
    -1 & 0 \\
\end{bmatrix} = \begin{bmatrix}
    0 & 2 \\
    0 & 1 \\
    1 & 2 \\
    1 & 3 \\
\end{bmatrix}
$$

The first matrix is the data points, whereas the second matrix is the weights, where a column is a set of weights. The resulting matrix is the predicted $y$'s for a set of weights. Alternatively, we can say $\hat{y}_{ij}$ is the prediction of $y$ for datapoint $i$ with weights from column $j$.

Now we compute the difference

$$
\Delta y = y - \hat{y} = \begin{bmatrix}
    2 \\
    0 \\
    3 \\
    2 \\
\end{bmatrix} - \begin{bmatrix}
    0 & 2 \\
    0 & 1 \\
    1 & 2 \\
    1 & 3 \\
\end{bmatrix} = \begin{bmatrix}
    2 & 0 \\
    0 & -1 \\
    2 & 1 \\
    1 & -1 \\
\end{bmatrix}
$$

Now that we have the difference matrix we simply iterate over it to calculate the mean squared error. We can do this by

$$
mse_j = 4^{-1}\Sigma_{i=0}^4 \Delta y_{ij}^2
$$

Remember that $j$ represents a column, and in this case $mse_j$ is the MSE for the set of weights in the $j$'th column.

$$
\begin{align*}
    mse_1 &= 4^{-1}(2^2 + 0^2 + 2^2 + 1^2) = 2.25
    mse_2 &= 4^{-1}(0 + (-1)^2 + 1^2 + (-1)^2) = 0.75
\end{align*}
$$

Which ultimately means, that the second column of weights is the most optimal for approximating the data points, i.e. $\{1, 1, 0\}$.

# Exercise 2

Consider a database of cars represented by the five training examples below. The target attribute *Acceptable*, which can have values *yes* and *no*, is to be predicted based on the other attributes of the car in question. These attributes indicate a) the age of the car (*Age* having values < 5 years and $\ge$ 5 years), b) the make of the car (*Make* having states *Toyota* and *Mazda*), c) the number of previous owners (*Owners* having values 1, 2 and 3), d) the number of kilometers (*Kilometers* having values > 150k and $\le$ 150k) and e) the number of doors (*Doors* having values 3 and 5).

![[table_05_02.png]]

- Calculate the entropy for the attribute *Owners*.

We use the formula:

$$
Entropy(P) = -\Sigma_{i=1}^n p_i * log_2(p_i)
$$
*Note that the slides use **p** to denote the set, however I find that a bit amgiguous.*

To construct $P$ we iterate over each element in the domain and count the occurences, and set $p_i$ to be the average probability for the occurence of the $i$'th element of the domain. By this definition

$$
\begin{align*}
    domain(Owners) &= \{1, 2, 3\} \\
    P &= (2*5^{-1}, 5^{-1}, 2*5^{-1}) \\
\end{align*}
$$
*Note that in the slides $P$ is defined as a sequence of single values and not a set of tuples. Somewhat confusing as a set (the domain) is unordered and therefore it should be impossible to associate any value in P to a specific element of the domain.*

Applying the formula to $P$ we get

$$
\begin{align*}
Entropy(P)  &= -2*5^{-1}log_2(2*5^{-1}) - 5^{-1}log_2(5^{-1}) - 2*5^{-1}log_2(2*5^{-1}) \\
            &= 1.52 \\
\end{align*}
$$

- Show the decision/classification tree that would be learned by the learning algorithm assuming that it is given the training examples in the database.













































